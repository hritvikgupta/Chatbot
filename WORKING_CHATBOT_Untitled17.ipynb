{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WORKING-CHATBOT Untitled17.ipynb",
      "provenance": [],
      "mount_file_id": "1LDJTGFmd5Gybhddd0WPu_h34JgIV1-s9",
      "authorship_tag": "ABX9TyNLfPX/G5gui8dTEtqsLg3+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hritvikgupta/Chatbot/blob/master/WORKING_CHATBOT_Untitled17.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0ETADP6yuBP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe117869-e1b4-4137-91b4-badedfb8dc15"
      },
      "source": [
        "## Importing the libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding,LSTM, Dropout, Dense\n",
        "from tensorflow.keras import utils"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUoW9ViRnRkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import requests, zipfile, io\n",
        "\n",
        "r = requests.get( 'https://github.com/shubham0204/Dataset_Archives/blob/master/chatbot_nlp.zip?raw=true' ) \n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9zJBCi0nT-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-S-ROknU8Ev",
        "colab_type": "text"
      },
      "source": [
        "## Importing and preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oZbzvE4WpH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ab289ddd-8848-4337-9fb4-f589c86fa1ae"
      },
      "source": [
        "import os\n",
        "import yaml\n",
        "\n",
        "\n",
        "dir_path = 'chatbot_nlp/data'\n",
        "files_list = os.listdir(dir_path + os.sep)\n",
        "\n",
        "questions = list()\n",
        "answers = list()\n",
        "for filepath in files_list:\n",
        "    stream = open( dir_path + os.sep + filepath , 'rb')\n",
        "    docs = yaml.safe_load(stream)\n",
        "    conversations = docs['conversations']\n",
        "    for con in conversations:\n",
        "        if len( con ) > 2 :\n",
        "            questions.append(con[0])\n",
        "            replies = con[ 1 : ]\n",
        "            ans = ''\n",
        "            for rep in replies:\n",
        "                ans += ' ' + rep\n",
        "            answers.append( ans )\n",
        "        elif len( con )> 1:\n",
        "            questions.append(con[0])\n",
        "            answers.append(con[1])\n",
        "\n",
        "answer_with_tags = list() \n",
        "for i in range(len(answers)):\n",
        "  if type(answers[i])==str:\n",
        "    answer_with_tags.append(answers[i])\n",
        "  else:\n",
        "    questions.pop(i)\n",
        "\n",
        "answers = list()    \n",
        "for i in range(len(answer_with_tags)):\n",
        "  answers.append('<START>' + answer_with_tags[i] + '<END>')\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(answers + questions)\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "print('vocab_Size: {}'.format(vocab_size))\\\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_Size: 1894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDnCZfFAYEGk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a93a2973-62c5-4035-d172-52105bc3757c"
      },
      "source": [
        "questions[:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'Hi', 'Greetings!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u__rFip1YLWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f9351a84-32cf-4f78-bf13-48bc33cbf14c"
      },
      "source": [
        "answers[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<START>Hi<END>',\n",
              " '<START>Hello<END>',\n",
              " '<START>Hello<END>',\n",
              " '<START>Greetings!<END>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6cremyQYQlw",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing the data\n",
        "* Tokenize and pad the questions\n",
        "* Tokenize and pad the answers. append start and end in all the sequences\n",
        "\n",
        "* Tokenize the pad answers remove the start in all sequences one hot encode the sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOKSRRlSXgAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from gensim.models import Word2Vec\n",
        "import re\n",
        "\n",
        "vocab = []\n",
        "for word in tokenizer.word_index:\n",
        "    vocab.append( word )\n",
        "\n",
        "def tokenize( sentences ):\n",
        "    tokens_list = []\n",
        "    vocabulary = []\n",
        "    for sentence in sentences:\n",
        "        sentence = str(sentence).lower()\n",
        "        sentence = re.sub( '[^a-zA-Z]', ' ', sentence )\n",
        "        tokens = sentence.split()\n",
        "        vocabulary += tokens\n",
        "        tokens_list.append( tokens )\n",
        "    return tokens_list , vocabulary\n",
        "\n",
        "p = tokenize( questions + answers )\n",
        "model = Word2Vec(p[0])\n",
        "\n",
        "embedding_matrix = np.zeros( ( vocab_size, 100 ) )\n",
        "#for i in range(len(tokenizer.word_index)):\n",
        " #   embedding_matrix[i] = model[vocab[i]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REiBQCg0YX_B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3cc78745-f489-47a5-e4de-a6cbc32ab215"
      },
      "source": [
        "\n",
        "ques = []\n",
        "\n",
        "tokenizer.fit_on_texts(questions)\n",
        "tokenized_seq = tokenizer.texts_to_sequences(questions)\n",
        "\n",
        "length_list = []\n",
        "for token_seq in tokenized_seq:\n",
        "  length_list.append(len(token_seq))\n",
        "max_input_length = np.array(length_list).max()\n",
        "print(\"Questions max lengths {}\".format(max_input_length))\n",
        "\n",
        "## Padding the sequence\n",
        "padded_question_lines = pad_sequences(tokenized_seq, maxlen = max_input_length, padding = 'post' )\n",
        "encoder_input_data =  np.array(padded_question_lines)\n",
        "print(\"Encoder input data shape{}\".format(encoder_input_data.shape))\n",
        "num_encoded_tokens = len(tokenizer.word_index)+1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Questions max lengths 22\n",
            "Encoder input data shape(564, 22)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rVr8oM55J2U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsaDjsMy3hOZ",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocessing input Data for the decoder(decoder_input_data)\n",
        "\n",
        "the decoder will be fed with the preprocessed Answers\n",
        "\n",
        "* Append<start> tag at the first position in each answer sequence\n",
        "* Append<END> tag at the last position in each answer sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqxxE0DLXeoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8EXp28A4Am7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "1000de93-bb40-4e41-d17d-93a8b5e253ec"
      },
      "source": [
        "## This is decoder input Data\n",
        "\n",
        "tokenized_ans_lines = tokenizer.texts_to_sequences(answers) \n",
        "\n",
        "length_list1 = list()\n",
        "for token_seq in tokenized_ans_lines:\n",
        "    length_list1.append( len( token_seq ))\n",
        "max_output_length = np.array( length_list1 ).max()\n",
        "print( 'Answer max length is {}'.format( max_output_length ))\n",
        "\n",
        "padded_ans_lines = pad_sequences( tokenized_ans_lines , maxlen=max_output_length, padding='post' )\n",
        "decoder_input_data = np.array( padded_ans_lines )\n",
        "print( 'Decoder input data shape -> {}'.format( decoder_input_data.shape ))\n",
        "\n",
        "ans_word_dict = tokenizer.word_index\n",
        "num_ans_tokens = len( ans_word_dict )+1\n",
        "print( 'Number of Answer tokens = {}'.format( num_ans_tokens))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Answer max length is 74\n",
            "Decoder input data shape -> (564, 74)\n",
            "Number of Answer tokens = 1894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTgYCJ-s62N2",
        "colab_type": "text"
      },
      "source": [
        "#### Preparing the Target Data For the decoder\n",
        "we take a copy of the tokenized_ans_lines and modify like this\n",
        "1. we remove the <start> tag which we appended earlier.\n",
        "2. convert the padded_ans_lines to the one hot vectors\n",
        "\n",
        "\n",
        "Or in general machine learning language this is y means labels we want to predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A40tM7X6684z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c5ea8579-8a34-4d13-ec7b-ef09001dae09"
      },
      "source": [
        "\n",
        "\n",
        "## This is decoder Output Data\n",
        "\n",
        "\n",
        "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
        "# Removing the start tag from all the tokenized nswers\n",
        "for i in range(len(tokenized_answers)):\n",
        "  tokenized_answers[i] = tokenized_answers[i][1: ]\n",
        "\n",
        "  \n",
        "padded_ans_lines = pad_sequences(tokenized_answers, maxlen = max_output_length, padding = 'post')\n",
        "onehot_lines = utils.to_categorical(padded_ans_lines, vocab_size)\n",
        "decoder_target_data = np.array(onehot_lines)\n",
        "print(\"Decoder_Target_input {}\".format(decoder_target_data.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder_Target_input (564, 74, 1894)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1eZkyQoYkTu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5d78014d-8cad-4263-dfc9-b1393405737b"
      },
      "source": [
        "num_encoded_tokens, num_ans_tokens, max_input_length, max_output_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1894, 1894, 22, 74)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1K2-fR-zY1-m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe1dd631-3feb-464a-9c0d-5c4f6146717a"
      },
      "source": [
        "\n",
        "decoder_target_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(564, 74, 1894)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SldbmvKH8i52",
        "colab_type": "text"
      },
      "source": [
        "###Defining and training the model\n",
        "\n",
        "* The model will have Embedding LSTM, and Dense Layer, and Dense Layers the basic Configuration as follow\n",
        "\n",
        "* 2 input layers: One for encoder input Data and other for decoder input Data\n",
        "\n",
        "* Embedding layer : for converting token vectors to fixed sized dense vectors\n",
        "\n",
        "* lstm layer: provide access to long short term cells\n",
        "\n",
        "\n",
        "## Working: \n",
        "1. The Encoder_input_data comes in the embedding layer (encoder_embedding)\n",
        "\n",
        "2. the output of the embedding layer goes to the lstm cell which produces 2 state vectors (h and c which are encoder states)\n",
        "\n",
        "3. These states are set in the lstm cell of the decoder\n",
        "4. the decoder_input data comes through the embedding layer\n",
        "5. the embedding goes in the lstm cell (which had the states) to produce sequences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yblej7nK8ptA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "283a220f-956e-4c08-cc21-2794246360ef"
      },
      "source": [
        "encoder_inputs = tf.keras.layers.Input(shape=(None, ))\n",
        "encoder_embedding = tf.keras.layers.Embedding(num_encoded_tokens, 200, mask_zero=True)(encoder_inputs)\n",
        "## State_h, state_c contains all the information about the questions or the input that is been given \n",
        "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(200, return_state=True)(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = tf.keras.layers.Input(shape=(None, ))\n",
        "decoder_embedding = tf.keras.layers.Embedding(num_ans_tokens, 200, mask_zero=True)(decoder_inputs)\n",
        "decoder_lstm = tf.keras.layers.LSTM(200, return_state=True, return_sequences=True )\n",
        "decoder_outputs, _,_ = decoder_lstm(decoder_embedding,initial_state = encoder_states)\n",
        "decoder_dense = tf.keras.layers.Dense(num_ans_tokens, activation=tf.keras.activations.softmax)\n",
        "output = decoder_dense(decoder_outputs)\n",
        "\n",
        "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
        "model.compile(optimizer = tf.keras.optimizers.RMSprop(), loss = 'categorical_crossentropy')\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 200)    378800      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    378800      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, 200), (None, 320800      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 200),  320800      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 1894)   380694      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,779,894\n",
            "Trainable params: 1,779,894\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SymdglYOOSFD",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "havBiCqbV7V_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f8d3f96-72bf-46f9-8267-6ec05f929d7c"
      },
      "source": [
        "num_encoded_tokens, max_input_length"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1894, 22)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EHJfa03TzGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ca8ab80-b9ad-4baf-90e6-63616f10f665"
      },
      "source": [
        "encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((564, 22), (564, 74), (564, 74, 1894))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnKKP2Y-1Kd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f662a22c-38ec-40e0-8e00-2a8b197ed2fc"
      },
      "source": [
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=25, epochs = 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 1.2250\n",
            "Epoch 2/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 1.0940\n",
            "Epoch 3/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 1.0583\n",
            "Epoch 4/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 1.0319\n",
            "Epoch 5/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 1.0090\n",
            "Epoch 6/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.9886\n",
            "Epoch 7/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.9673\n",
            "Epoch 8/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.9447\n",
            "Epoch 9/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.9225\n",
            "Epoch 10/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.9005\n",
            "Epoch 11/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.8794\n",
            "Epoch 12/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.8591\n",
            "Epoch 13/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.8397\n",
            "Epoch 14/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.8215\n",
            "Epoch 15/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.8040\n",
            "Epoch 16/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.7866\n",
            "Epoch 17/50\n",
            "23/23 [==============================] - 1s 56ms/step - loss: 0.7710\n",
            "Epoch 18/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.7551\n",
            "Epoch 19/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.7395\n",
            "Epoch 20/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.7239\n",
            "Epoch 21/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.7085\n",
            "Epoch 22/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.6937\n",
            "Epoch 23/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.6781\n",
            "Epoch 24/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.6637\n",
            "Epoch 25/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.6487\n",
            "Epoch 26/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.6348\n",
            "Epoch 27/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.6204\n",
            "Epoch 28/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.6060\n",
            "Epoch 29/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.5925\n",
            "Epoch 30/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.5781\n",
            "Epoch 31/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.5647\n",
            "Epoch 32/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.5521\n",
            "Epoch 33/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.5378\n",
            "Epoch 34/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.5252\n",
            "Epoch 35/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.5115\n",
            "Epoch 36/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.4993\n",
            "Epoch 37/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.4857\n",
            "Epoch 38/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.4734\n",
            "Epoch 39/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.4608\n",
            "Epoch 40/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.4479\n",
            "Epoch 41/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.4366\n",
            "Epoch 42/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.4235\n",
            "Epoch 43/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.4121\n",
            "Epoch 44/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.4003\n",
            "Epoch 45/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.3891\n",
            "Epoch 46/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.3790\n",
            "Epoch 47/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.3660\n",
            "Epoch 48/50\n",
            "23/23 [==============================] - 1s 53ms/step - loss: 0.3546\n",
            "Epoch 49/50\n",
            "23/23 [==============================] - 1s 55ms/step - loss: 0.3440\n",
            "Epoch 50/50\n",
            "23/23 [==============================] - 1s 54ms/step - loss: 0.3346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f049d136358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjE7QGoJPpgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zu-cf4VLK83",
        "colab_type": "text"
      },
      "source": [
        "## INferencing the model\n",
        "1) we create the inferencing in predicting the answers\n",
        "\n",
        "* Encoder Inference model takes the questions as inputs and outputs the LSTM states(h and c)\n",
        "\n",
        "* decoder inference model takes 2 inputs one are the LSTM states(output of the encoder model), second os the question input one not having the start tag it will output the answers which we fed to the encoder model and its state values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC-leec8QXyf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_inference_models():\n",
        "  encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
        "\n",
        "  decoder_state_input_h = tf.keras.layers.Input(shape = (200, ))\n",
        "  decoder_state_input_c = tf.keras.layers.Input(shape = (200, ))\n",
        "  \n",
        "  decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        " \n",
        "  decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_state_inputs)\n",
        "  decoder_states = [state_h, state_c]\n",
        "  decoder_outputs = decoder_dense(decoder_outputs)\n",
        "  decoder_model = tf.keras.models.Model(\n",
        "                  [decoder_inputs] + decoder_state_inputs,\n",
        "                  [decoder_outputs]+ decoder_states )\n",
        "  \n",
        "  return encoder_model, decoder_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfZowbBYSgSf",
        "colab_type": "text"
      },
      "source": [
        "## Talking to our bot\n",
        "First we Define a Method str_to_tokens which converts the str questions to the string of integers ith padding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qWRz1MIVj-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def str_to_tokens(sentence : str):\n",
        "  words  = sentence.lower().split()\n",
        "  tokens_list = list()\n",
        "  for word in words:\n",
        "    tokens_list.append(tokenizer.word_index[word])\n",
        "  return pad_sequences([tokens_list], maxlen = max_input_length)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwGcHsFLWhA-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "058b4a08-e58c-4aef-b2d3-f0e2044dd066"
      },
      "source": [
        "enc_model , dec_model = make_inference_models()\n",
        "\n",
        "for _ in range(10):\n",
        "    states_values = enc_model.predict( str_to_tokens( input( 'Enter question : ' ) ) )\n",
        "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
        "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
        "    stop_condition = False\n",
        "    decoded_translation = ''\n",
        "    while not stop_condition :\n",
        "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
        "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
        "        sampled_word = None\n",
        "        for word , index in tokenizer.word_index.items() :\n",
        "            if sampled_word_index == index :\n",
        "                decoded_translation += ' {}'.format( word )\n",
        "                sampled_word = word\n",
        "        \n",
        "        if sampled_word == 'end' or len(decoded_translation.split()) > max_input_length:\n",
        "            stop_condition = True\n",
        "            \n",
        "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "        states_values = [ h , c ] \n",
        "\n",
        "    print( decoded_translation )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " i like to a lot of the universe end\n",
            " my favorite subject is 2001 end\n",
            " i am not myself end\n",
            " i like to a lot of the computer end\n",
            " i like to a lot of the computer end\n",
            " no i am not as myself as i am not yet end\n",
            " i like to a lot of the computer end\n",
            " i like to a lot of the computer end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1c1Em78Wf4v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "how shknotokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaKztqLBXQWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}